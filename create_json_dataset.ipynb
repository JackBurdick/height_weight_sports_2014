{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse data from raw files and create .json with target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "# import h5py # TODO: write data to hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# config\n",
    "INPUT_DIR = 'raw_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Obtain Relevant Data\n",
    "In this case, I'm only interested in the height and weight from each sport\n",
    "we'll;\n",
    "1. read in the data\n",
    "2. parse relevant information\n",
    "3. write the data to a new file for future use\n",
    "\n",
    "Writing the cleaned data to a new file may not be necessary, but if we were dealing with massive ammounts of data, depending on our machiene, it may not be possible to store/read all the data at once.  In this example, we'll loop each file one at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create dictionary mapping of header information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nhl_ht_wt.csv': {'Weight': 4, 'Height': 3}, 'epl_2015_ht_wt.csv': {'Weight': 6, 'Height': 5}, 'nba_ht_wt.csv': {'Weight': 3, 'Height': 2}}\n"
     ]
    }
   ],
   "source": [
    "# loop directory, create header details dict for each file\n",
    "ATTRIBUTES = ['Height', 'Weight']\n",
    "fileDict_attributeMapping = {}\n",
    "for datapath in os.listdir(INPUT_DIR):\n",
    "    with open(os.path.join(INPUT_DIR, datapath), 'rb') as csvfile:\n",
    "        fileDict_attributeMapping[datapath] = {}\n",
    "        \n",
    "        # read header row, decode from bytes\n",
    "        fileHeader = csvfile.readline()\n",
    "        fileHeader = fileHeader.decode('ascii')\n",
    "        \n",
    "        # clean trailing return and newline char\n",
    "        fileHeader = fileHeader.rstrip(\"\\r\\n\")\n",
    "\n",
    "        # split into contents\n",
    "        fileHeader_contents = fileHeader.split(\",\")\n",
    "        \n",
    "        # create index mapping\n",
    "        # loop attributes, create mapping for each item\n",
    "        for attribute in ATTRIBUTES:\n",
    "            for index, item in enumerate(fileHeader_contents):\n",
    "                if item == attribute:\n",
    "                    fileDict_attributeMapping[datapath][attribute] = index\n",
    "\n",
    "# ensure we've created our mapping        \n",
    "print(fileDict_attributeMapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# loop directories and read in data\n",
    "# NOTE: this reads the entire file into memory\n",
    "# > and could be modified at a later date to read line by line \n",
    "\n",
    "# this will hold the data (in the intended format) to write to the `.json` file\n",
    "data_dict = {}\n",
    "\n",
    "for datapath in os.listdir(INPUT_DIR):\n",
    "    # create dataset name from file path ('nba', 'nhl', 'epl)\n",
    "    groupName = datapath[:3]\n",
    "    \n",
    "    # write relevant data from each file in the root directory\n",
    "    curDict = fileDict_attributeMapping[datapath]\n",
    "    with open(os.path.join(INPUT_DIR, datapath), encoding='latin-1') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        \n",
    "        firstLine = True\n",
    "        listOfData = []\n",
    "        for row in reader:\n",
    "            # add relevant data to a list from each row (in this case an individual player)\n",
    "            individualDataDict = {}\n",
    "            \n",
    "            # skip the first (header) line\n",
    "            if firstLine:\n",
    "                firstLine = False\n",
    "                continue\n",
    "             \n",
    "            # collect data from each target attribute\n",
    "            for attribute in ATTRIBUTES:\n",
    "                curVal = row[curDict[attribute]]\n",
    "                individualDataDict[attribute] = curVal\n",
    "            \n",
    "            listOfData.append(individualDataDict)\n",
    "            \n",
    "    data_dict[groupName] = listOfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write to json file\n",
    "# doc: https://www.quantifiedcode.com/knowledge-base/maintainability/Use%20dump%20instead%20of%20dumps%20for%20json%20files/55b5QncE\n",
    "with open(\"ht_wt_data_2014.json\", \"w\") as json_file:\n",
    "    json.dump(data_dict, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## All data was collected and stored in a json file\n",
    "Next we'll explore this data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
